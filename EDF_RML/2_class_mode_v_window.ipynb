{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c90f743",
   "metadata": {},
   "source": [
    "# Model\n",
    "## Wav2Vec2 2Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b346f12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import os, json, time, re, random, numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import torchaudio\n",
    "    HAVE_TA = True\n",
    "except Exception:\n",
    "    HAVE_TA = False\n",
    "    print(\"[WARN] torchaudio not found, augmentation disabled.\")\n",
    "\n",
    "import pyedflib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_recall_fscore_support,\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    log_loss, average_precision_score\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor, Wav2Vec2ForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c135e2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060\n",
      "CUDA Version: 12.1\n"
     ]
    }
   ],
   "source": [
    "# Device and Reproducibility Setup\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d396ed1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Model: facebook/wav2vec2-base\n",
      "  Batch Size: 4\n",
      "  Epochs: 5\n",
      "  Learning Rate: 5e-06\n",
      "  Max Audio Length: 8s\n",
      "  Use Class Weights: True\n"
     ]
    }
   ],
   "source": [
    "# Binary Classification Configuration - FIXED\n",
    "CSV_PATH = r\"C:\\V89\\Snore_Apnea_Analyze\\EDF_RML\\data_csv\\respiratory_plus_normal.csv\"\n",
    "EDF_ROOT = r\"C:\\V89\\data2\"\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_NAME = \"facebook/wav2vec2-base\"\n",
    "MODEL_TAG = \"wav2vec2-binary-apnea-vs-normal-v2-fixed\"\n",
    "\n",
    "# Training Parameters - OPTIMIZED\n",
    "SAMPLE_RATE = 16000\n",
    "MAX_SECONDS = 8\n",
    "BATCH_SIZE = 4  # à¸¥à¸” batch size\n",
    "EPOCHS = 5      # à¸¥à¸”à¹€à¸›à¹‡à¸™ 5 epochs\n",
    "LR = 5e-6      # à¸¥à¸” learning rate\n",
    "WARMUP_RATIO = 0.05  # à¸¥à¸” warmup\n",
    "\n",
    "# Training Options\n",
    "FREEZE_BASE = False\n",
    "AUGMENT = True\n",
    "USE_CLASS_WEIGHTS = True\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning Rate: {LR}\")\n",
    "print(f\"  Max Audio Length: {MAX_SECONDS}s\")\n",
    "print(f\"  Use Class Weights: {USE_CLASS_WEIGHTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05f947b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original dataset...\n",
      "Original class distribution:\n",
      "type\n",
      "Normal              912\n",
      "ObstructiveApnea    649\n",
      "Hypopnea            398\n",
      "MixedApnea           72\n",
      "CentralApnea         38\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Binary class distribution:\n",
      "type\n",
      "Apnea     1157\n",
      "Normal     912\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Binary dataset saved to: C:\\V89\\Snore_Apnea_Analyze\\EDF_RML\\data_csv\\apnea_vs_normal.csv\n"
     ]
    }
   ],
   "source": [
    "# Create Binary Dataset (Apnea vs Normal)\n",
    "print(\"Loading original dataset...\")\n",
    "df_original = pd.read_csv(CSV_PATH)\n",
    "print(\"Original class distribution:\")\n",
    "print(df_original['type'].value_counts())\n",
    "\n",
    "def create_binary_classes(df):\n",
    "    \"\"\"Convert multi-class to binary: Apnea vs Normal\"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Combine all respiratory events into \"Apnea\"\n",
    "    apnea_types = ['ObstructiveApnea', 'Hypopnea', 'MixedApnea', 'CentralApnea']\n",
    "    df_new.loc[df_new['type'].isin(apnea_types), 'type'] = 'Apnea'\n",
    "    # Normal remains \"Normal\"\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Create binary dataset\n",
    "df_binary = create_binary_classes(df_original)\n",
    "print(\"\\nBinary class distribution:\")\n",
    "print(df_binary['type'].value_counts())\n",
    "\n",
    "# Save binary dataset\n",
    "binary_csv_path = r\"C:\\V89\\Snore_Apnea_Analyze\\EDF_RML\\data_csv\\apnea_vs_normal.csv\"\n",
    "df_binary.to_csv(binary_csv_path, index=False)\n",
    "print(f\"\\nBinary dataset saved to: {binary_csv_path}\")\n",
    "\n",
    "# Update CSV_PATH to use binary dataset\n",
    "CSV_PATH = binary_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4d48bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sleep Apnea Dataset Class\n",
    "class SleepApneaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for loading EDF audio segments with respiratory event labels.\n",
    "    Handles EDF file resolution and audio channel selection automatically.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, edf_root: str, sample_rate: int = 16000,\n",
    "                 prefer_audio_channels=(\"sound\",\"snore\",\"tracheal\",\"microphone\",\"audio\",\"throat\")):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.edf_root = Path(edf_root)\n",
    "        self.prefer_audio_channels = tuple(s.lower() for s in prefer_audio_channels)\n",
    "\n",
    "        df = df.copy()\n",
    "        df[\"pid_str\"] = df[\"patient_id\"].astype(str)\n",
    "        df[\"pid_unpad\"] = df[\"pid_str\"].str.lstrip(\"0\")\n",
    "        df[\"pid_pad8\"] = df[\"pid_unpad\"].str.zfill(8)\n",
    "        df[\"seg3\"] = df[\"segment_index\"].astype(int).map(lambda x: f\"{x:03d}\")\n",
    "\n",
    "        def resolve_edf_path(row):\n",
    "            \"\"\"Find matching EDF file for this row\"\"\"\n",
    "            pid_unpad, pid_pad8, seg3 = row[\"pid_unpad\"], row[\"pid_pad8\"], row[\"seg3\"]\n",
    "            patterns = [\n",
    "                f\"*{pid_pad8}*{seg3}*.edf\",\n",
    "                f\"*{pid_unpad}*{seg3}*.edf\",\n",
    "                f\"*{pid_pad8}*.edf\",\n",
    "                f\"*{pid_unpad}*.edf\",\n",
    "            ]\n",
    "            for pat in patterns:\n",
    "                hits = list(self.edf_root.rglob(pat))\n",
    "                if len(hits) == 1:\n",
    "                    return hits[0]\n",
    "                if len(hits) > 1:\n",
    "                    # Rank by segment match and audio-related keywords\n",
    "                    ranked = sorted(hits, key=lambda p: (\n",
    "                        0 if re.search(rf\"{seg3}\\b\", p.stem) else 1,\n",
    "                        0 if re.search(r\"(snore|sound|trach|mic|psg|audio|throat)\", p.stem.lower()) else 1,\n",
    "                        len(p.as_posix())\n",
    "                    ))\n",
    "                    return ranked[0]\n",
    "            return None\n",
    "\n",
    "        # Resolve EDF paths and filter out missing files\n",
    "        df[\"edf_path\"] = df.apply(resolve_edf_path, axis=1)\n",
    "        missing = df[\"edf_path\"].isna().sum()\n",
    "        if missing > 0:\n",
    "            print(f\"[WARNING] Skipping {missing} rows without matching EDF files\")\n",
    "        df = df.dropna(subset=[\"edf_path\"]).reset_index(drop=True)\n",
    "\n",
    "        self.df = df\n",
    "        self.labels = sorted(self.df['type'].unique().tolist())\n",
    "        self.label_encoder = LabelEncoder().fit(self.labels)\n",
    "        \n",
    "        print(f\"Dataset initialized:\")\n",
    "        print(f\"  Total samples: {len(self.df)}\")\n",
    "        print(f\"  Classes: {self.labels}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _pick_audio_channel(self, edf_reader):\n",
    "        \"\"\"Select the best audio channel from EDF file\"\"\"\n",
    "        labels = [s.lower() for s in edf_reader.getSignalLabels()]\n",
    "        for i, name in enumerate(labels):\n",
    "            if any(key in name for key in self.prefer_audio_channels):\n",
    "                return i\n",
    "        return 0  # fallback to first channel\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        edf_path = Path(row[\"edf_path\"])\n",
    "        start_sec = float(row['segment_local_start_sec'])\n",
    "        duration_sec = float(row['duration_sec'])\n",
    "        label = self.label_encoder.transform([row['type']])[0]\n",
    "\n",
    "        # Load audio segment from EDF\n",
    "        edf_reader = pyedflib.EdfReader(str(edf_path))\n",
    "        ch_idx = self._pick_audio_channel(edf_reader)\n",
    "        signal = edf_reader.readSignal(ch_idx)\n",
    "        fs = edf_reader.getSampleFrequency(ch_idx)\n",
    "        edf_reader.close()\n",
    "\n",
    "        # Extract segment\n",
    "        start_sample = max(0, int(start_sec * fs))\n",
    "        end_sample = min(int((start_sec + duration_sec) * fs), len(signal))\n",
    "        audio = signal[start_sample:end_sample]\n",
    "\n",
    "        # Convert to tensor and resample if needed\n",
    "        x = torch.tensor(audio, dtype=torch.float32)\n",
    "        if fs != self.sample_rate:\n",
    "            if not HAVE_TA:\n",
    "                raise RuntimeError(\"torchaudio required for resampling\")\n",
    "            x = torchaudio.transforms.Resample(fs, self.sample_rate)(x)\n",
    "\n",
    "        return {\"audio\": x, \"label\": int(label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30608f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading binary dataset...\n",
      "[WARNING] Skipping 245 rows without matching EDF files\n",
      "Dataset initialized:\n",
      "  Total samples: 1824\n",
      "  Classes: ['Apnea', 'Normal']\n",
      "Binary dataset loaded successfully!\n",
      "Classes: [np.str_('Apnea'), np.str_('Normal')]\n",
      "Number of classes: 2\n",
      "Loaded existing data splits\n",
      "\n",
      "Dataset splits:\n",
      "  Train: 1459 samples\n",
      "  Val:   182 samples\n",
      "  Test:  183 samples\n",
      "\n",
      "Class distributions:\n",
      "Train: {np.str_('Apnea'): 729, np.str_('Normal'): 730}\n",
      "Val: {np.str_('Normal'): 88, np.str_('Apnea'): 94}\n",
      "Test: {np.str_('Apnea'): 89, np.str_('Normal'): 94}\n",
      "[WARNING] Skipping 245 rows without matching EDF files\n",
      "Dataset initialized:\n",
      "  Total samples: 1824\n",
      "  Classes: ['Apnea', 'Normal']\n",
      "Binary dataset loaded successfully!\n",
      "Classes: [np.str_('Apnea'), np.str_('Normal')]\n",
      "Number of classes: 2\n",
      "Loaded existing data splits\n",
      "\n",
      "Dataset splits:\n",
      "  Train: 1459 samples\n",
      "  Val:   182 samples\n",
      "  Test:  183 samples\n",
      "\n",
      "Class distributions:\n",
      "Train: {np.str_('Apnea'): 729, np.str_('Normal'): 730}\n",
      "Val: {np.str_('Normal'): 88, np.str_('Apnea'): 94}\n",
      "Test: {np.str_('Apnea'): 89, np.str_('Normal'): 94}\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset and Create Splits\n",
    "print(\"Loading binary dataset...\")\n",
    "df_binary = pd.read_csv(CSV_PATH)\n",
    "dataset_binary = SleepApneaDataset(df_binary, edf_root=EDF_ROOT, sample_rate=SAMPLE_RATE)\n",
    "\n",
    "label_names_binary = list(dataset_binary.label_encoder.classes_)\n",
    "n_classes_binary = len(label_names_binary)\n",
    "\n",
    "print(f\"Binary dataset loaded successfully!\")\n",
    "print(f\"Classes: {label_names_binary}\")\n",
    "print(f\"Number of classes: {n_classes_binary}\")\n",
    "\n",
    "# Create Train/Val/Test Splits\n",
    "SPLIT_FILE_BINARY = Path(\"./splits/binary_data_split.json\")\n",
    "\n",
    "if SPLIT_FILE_BINARY.exists():\n",
    "    # Load existing splits\n",
    "    with open(SPLIT_FILE_BINARY, \"r\") as f:\n",
    "        idx_binary = json.load(f)\n",
    "    train_idx_binary = idx_binary[\"train\"]\n",
    "    val_idx_binary = idx_binary[\"val\"] \n",
    "    test_idx_binary = idx_binary[\"test\"]\n",
    "    print(\"Loaded existing data splits\")\n",
    "else:\n",
    "    # Create new splits\n",
    "    g = torch.Generator().manual_seed(SEED)\n",
    "    N_binary = len(dataset_binary)\n",
    "    perm_binary = torch.randperm(N_binary, generator=g).tolist()\n",
    "    \n",
    "    train_ratio, val_ratio = 0.8, 0.1\n",
    "    n_train_binary = int(train_ratio * N_binary)\n",
    "    n_val_binary = int(val_ratio * N_binary)\n",
    "    \n",
    "    train_idx_binary = perm_binary[:n_train_binary]\n",
    "    val_idx_binary = perm_binary[n_train_binary:n_train_binary + n_val_binary]\n",
    "    test_idx_binary = perm_binary[n_train_binary + n_val_binary:]\n",
    "    \n",
    "    # Save splits\n",
    "    SPLIT_FILE_BINARY.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(SPLIT_FILE_BINARY, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"train\": train_idx_binary, \n",
    "            \"val\": val_idx_binary, \n",
    "            \"test\": test_idx_binary\n",
    "        }, f, indent=2)\n",
    "    print(\"Created and saved new data splits\")\n",
    "\n",
    "# Create subset datasets\n",
    "train_ds_binary = Subset(dataset_binary, train_idx_binary)\n",
    "val_ds_binary = Subset(dataset_binary, val_idx_binary)\n",
    "test_ds_binary = Subset(dataset_binary, test_idx_binary)\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"  Train: {len(train_ds_binary)} samples\")\n",
    "print(f\"  Val:   {len(val_ds_binary)} samples\")\n",
    "print(f\"  Test:  {len(test_ds_binary)} samples\")\n",
    "\n",
    "# Quick class distribution check\n",
    "def check_distribution(dataset, indices, name):\n",
    "    labels = [dataset.label_encoder.transform([dataset.df.iloc[i]['type']])[0] for i in indices]\n",
    "    from collections import Counter\n",
    "    cnt = Counter(labels)\n",
    "    cnt_named = {dataset.label_encoder.classes_[k]: v for k, v in cnt.items()}\n",
    "    print(f\"{name}: {cnt_named}\")\n",
    "\n",
    "print(\"\\nClass distributions:\")\n",
    "check_distribution(dataset_binary, train_idx_binary, \"Train\")\n",
    "check_distribution(dataset_binary, val_idx_binary, \"Val\")\n",
    "check_distribution(dataset_binary, test_idx_binary, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a73ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Wav2Vec2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\V89\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:334: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Trainable parameters: 94,569,090\n",
      "Model classes: [np.str_('Apnea'), np.str_('Normal')]\n",
      "\n",
      "Data loaders created:\n",
      "  Train batches: 365\n",
      "  Val batches: 46\n",
      "  Test batches: 46\n"
     ]
    }
   ],
   "source": [
    "# Wav2Vec2 Data Collator\n",
    "class Wav2Vec2Collator:\n",
    "    \"\"\"Prepare raw waveforms for Wav2Vec2 (padding + attention mask + optional augmentation)\"\"\"\n",
    "    def __init__(self, processor, sr=16000, max_seconds=8, augment=False):\n",
    "        self.processor = processor\n",
    "        self.sr = sr\n",
    "        self.max_len = int(max_seconds * sr)\n",
    "        self.augment = augment and HAVE_TA\n",
    "\n",
    "    def _augment(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply light audio augmentations\"\"\"\n",
    "        if not self.augment:\n",
    "            return x\n",
    "        \n",
    "        # Gaussian noise (50% chance)\n",
    "        if random.random() < 0.5:\n",
    "            noise_std = 0.005 * (x.abs().mean().item() + 1e-6)\n",
    "            x = x + torch.randn_like(x) * noise_std\n",
    "        \n",
    "        # Random gain (30% chance)\n",
    "        if random.random() < 0.3:\n",
    "            gain_db = random.uniform(-3.0, 3.0)\n",
    "            x = x * (10.0 ** (gain_db / 20.0))\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        waves, labels = [], []\n",
    "        \n",
    "        for sample in batch:\n",
    "            w = sample[\"audio\"]\n",
    "            if isinstance(w, np.ndarray):\n",
    "                w = torch.from_numpy(w)\n",
    "            w = w.float().view(-1)\n",
    "\n",
    "            # Crop long clips randomly\n",
    "            if len(w) > self.max_len:\n",
    "                start = random.randint(0, len(w) - self.max_len)\n",
    "                w = w[start:start + self.max_len]\n",
    "\n",
    "            # Peak normalize\n",
    "            peak = w.abs().max()\n",
    "            if peak > 0:\n",
    "                w = w / peak\n",
    "\n",
    "            # Apply augmentation\n",
    "            w = self._augment(w)\n",
    "\n",
    "            waves.append(w.numpy())\n",
    "            labels.append(int(sample[\"label\"]))\n",
    "\n",
    "        # Process with Wav2Vec2 processor\n",
    "        inputs = self.processor(\n",
    "            waves,\n",
    "            sampling_rate=self.sr,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=False\n",
    "        )\n",
    "        inputs[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "        return inputs\n",
    "\n",
    "# Setup Model and Processor\n",
    "print(\"Setting up Wav2Vec2 model...\")\n",
    "processor_binary = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model_binary = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=n_classes_binary,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "# Configure label mappings\n",
    "model_binary.config.id2label = {i: name for i, name in enumerate(label_names_binary)}\n",
    "model_binary.config.label2id = {name: i for i, name in enumerate(label_names_binary)}\n",
    "\n",
    "model_binary.to(device)\n",
    "\n",
    "# Count parameters\n",
    "trainable_params = sum(p.numel() for p in model_binary.parameters() if p.requires_grad)\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model classes: {label_names_binary}\")\n",
    "\n",
    "# Create Data Loaders\n",
    "collate_fn_binary = Wav2Vec2Collator(processor_binary, sr=SAMPLE_RATE, max_seconds=MAX_SECONDS, augment=AUGMENT)\n",
    "\n",
    "NUM_WORKERS = 0  # Keep 0 for Windows compatibility\n",
    "PIN_MEMORY = (device.type == \"cuda\")\n",
    "\n",
    "train_loader_binary = DataLoader(\n",
    "    train_ds_binary, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    collate_fn=collate_fn_binary, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    ")\n",
    "val_loader_binary = DataLoader(\n",
    "    val_ds_binary, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    collate_fn=collate_fn_binary, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    ")\n",
    "test_loader_binary = DataLoader(\n",
    "    test_ds_binary, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    collate_fn=collate_fn_binary, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    ")\n",
    "\n",
    "print(f\"\\nData loaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader_binary)}\")\n",
    "print(f\"  Val batches: {len(val_loader_binary)}\")\n",
    "print(f\"  Test batches: {len(test_loader_binary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2b77e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Functions\n",
    "def evaluate_model(model, loader, label_encoder, device, return_arrays=False):\n",
    "    \"\"\"Comprehensive model evaluation with metrics calculation\"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_proba_chunks = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(\n",
    "                input_values=batch[\"input_values\"],\n",
    "                attention_mask=batch.get(\"attention_mask\", None)\n",
    "            ).logits\n",
    "            \n",
    "            proba = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            y_proba_chunks.append(proba)\n",
    "            y_pred.extend(np.argmax(proba, axis=1))\n",
    "            y_true.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_proba = np.vstack(y_proba_chunks) if len(y_proba_chunks) > 0 else np.zeros((0, len(label_encoder.classes_)))\n",
    "\n",
    "    class_names = list(label_encoder.classes_)\n",
    "    n_classes = len(class_names)\n",
    "    \n",
    "    # Handle case where not all classes appear in predictions\n",
    "    unique_labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=unique_labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_true, y_pred) if len(y_true) else np.nan\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0) if len(y_true) else np.nan\n",
    "    f1_weighted = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0) if len(y_true) else np.nan\n",
    "    prec_macro, rec_macro, _, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0) if len(y_true) else (np.nan, np.nan, None, None)\n",
    "\n",
    "    # Balanced accuracy\n",
    "    cm_float = cm.astype(float) if cm.size else np.zeros((len(unique_labels), len(unique_labels)), float)\n",
    "    row_sums = cm_float.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1.0\n",
    "    bal_acc = (cm_float / row_sums).diagonal().mean() if cm.size else np.nan\n",
    "\n",
    "    # ROC AUC and PR AUC\n",
    "    try:\n",
    "        roc_auc_macro = roc_auc_score(y_true, y_proba, multi_class=\"ovr\", average=\"macro\")\n",
    "    except Exception:\n",
    "        roc_auc_macro = np.nan\n",
    "    \n",
    "    try:\n",
    "        pr_auc_macro = average_precision_score(np.eye(n_classes)[y_true], y_proba, average=\"macro\")\n",
    "    except Exception:\n",
    "        pr_auc_macro = np.nan\n",
    "    \n",
    "    try:\n",
    "        ll = log_loss(y_true, y_proba, labels=list(range(n_classes)))\n",
    "    except Exception:\n",
    "        ll = np.nan\n",
    "\n",
    "    # Classification report\n",
    "    report_dict = classification_report(\n",
    "        y_true, y_pred, \n",
    "        labels=unique_labels,\n",
    "        target_names=[class_names[i] for i in unique_labels],\n",
    "        output_dict=True, zero_division=0\n",
    "    ) if len(y_true) else {}\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": acc,\n",
    "        \"balanced_accuracy\": bal_acc,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"precision_macro\": prec_macro,\n",
    "        \"recall_macro\": rec_macro,\n",
    "        \"roc_auc_macro\": roc_auc_macro,\n",
    "        \"pr_auc_macro\": pr_auc_macro,\n",
    "        \"log_loss\": ll,\n",
    "        \"support\": int(len(y_true))\n",
    "    }\n",
    "    \n",
    "    out = (metrics, report_dict, cm, class_names)\n",
    "    if return_arrays:\n",
    "        return out + (y_true, y_pred, y_proba)\n",
    "    return out\n",
    "\n",
    "def save_confusion_matrix(cm, class_names, save_path, normalize=True, title=None):\n",
    "    \"\"\"Save confusion matrix visualization\"\"\"\n",
    "    cm_plot = cm.astype(float)\n",
    "    if normalize and cm_plot.size:\n",
    "        row_sums = cm_plot.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1.0\n",
    "        cm_plot = cm_plot / row_sums\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm_plot, interpolation='nearest', cmap='Blues')\n",
    "    plt.title(title or \"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted Class\")\n",
    "    plt.ylabel(\"True Class\")\n",
    "    \n",
    "    ticks = np.arange(len(class_names))\n",
    "    plt.xticks(ticks, class_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(ticks, class_names)\n",
    "\n",
    "    fmt = \".3f\" if normalize else \"d\"\n",
    "    thresh = cm_plot.max() / 2. if cm_plot.size else 0.5\n",
    "    \n",
    "    for i in range(cm_plot.shape[0]):\n",
    "        for j in range(cm_plot.shape[1]):\n",
    "            val = cm_plot[i, j] if normalize else int(cm[i, j])\n",
    "            color = \"white\" if (cm_plot[i, j] if cm_plot.size else 0) > thresh else \"black\"\n",
    "            plt.text(j, i, format(val, fmt), ha=\"center\", va=\"center\", color=color)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def save_classification_report(report_dict, save_csv_path):\n",
    "    \"\"\"Save classification report to CSV\"\"\"\n",
    "    if not report_dict:\n",
    "        return None\n",
    "    df = pd.DataFrame(report_dict).T\n",
    "    Path(save_csv_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(save_csv_path, index=True)\n",
    "    return df\n",
    "\n",
    "def append_metrics_row(results_csv, model_name, split_name, metrics, extras=None):\n",
    "    \"\"\"Append metrics to results CSV file\"\"\"\n",
    "    row = {\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"model\": model_name,\n",
    "        \"split\": split_name,\n",
    "        **metrics\n",
    "    }\n",
    "    if extras:\n",
    "        row.update(extras)\n",
    "    \n",
    "    if os.path.exists(results_csv):\n",
    "        df = pd.read_csv(results_csv)\n",
    "        df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame([row])\n",
    "    \n",
    "    Path(results_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(results_csv, index=False)\n",
    "    return df.tail(1)\n",
    "\n",
    "print(\"Evaluation functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75ca70d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using smoothed class weights: {np.str_('Apnea'): np.float64(1.0), np.str_('Normal'): np.float64(1.0)}\n",
      "Label smoothing: 0.1\n",
      "Training setup complete!\n",
      "  Total training steps: 1825\n",
      "  Warmup steps: 91\n",
      "  Using AMP: True\n",
      "  Output directory: eval_out\\wav2vec2-binary-apnea-vs-normal-v2-fixed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abcde\\AppData\\Local\\Temp\\ipykernel_24968\\2833564887.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_binary = torch.cuda.amp.GradScaler(enabled=use_amp_binary)\n"
     ]
    }
   ],
   "source": [
    "# Training Setup - FIXED VERSION\n",
    "OUT_DIR_BINARY = Path(f\"./eval_out/{MODEL_TAG}\")\n",
    "OUT_DIR_BINARY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training components\n",
    "total_steps_binary = len(train_loader_binary) * EPOCHS\n",
    "warmup_steps_binary = int(total_steps_binary * WARMUP_RATIO)\n",
    "\n",
    "# ===== FIX 1: à¹ƒà¸Šà¹‰ AdamW vá»›i weight decay =====\n",
    "optimizer_binary = torch.optim.AdamW(\n",
    "    model_binary.parameters(), \n",
    "    lr=LR, \n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "scheduler_binary = get_linear_schedule_with_warmup(optimizer_binary, warmup_steps_binary, total_steps_binary)\n",
    "\n",
    "# AMP Setup\n",
    "use_amp_binary = (device.type == \"cuda\")\n",
    "scaler_binary = torch.cuda.amp.GradScaler(enabled=use_amp_binary)\n",
    "\n",
    "# ===== FIX 2: à¸›à¸£à¸±à¸š Class Weights =====\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    all_y_binary = dataset_binary.label_encoder.transform(dataset_binary.df['type'])\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    classes_binary = np.arange(n_classes_binary)\n",
    "    weights_binary = compute_class_weight(class_weight='balanced', classes=classes_binary, y=all_y_binary)\n",
    "    \n",
    "    # à¸¥à¸”à¸„à¸§à¸²à¸¡à¸£à¸¸à¸™à¹à¸£à¸‡à¸‚à¸­à¸‡ class weights\n",
    "    weights_binary = np.sqrt(weights_binary)\n",
    "    \n",
    "    class_weights_binary = torch.tensor(weights_binary, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # ===== FIX 3: à¹€à¸žà¸´à¹ˆà¸¡ Label Smoothing =====\n",
    "    ce_loss_binary = nn.CrossEntropyLoss(weight=class_weights_binary, label_smoothing=0.1)\n",
    "    print(f\"Using smoothed class weights: {dict(zip(label_names_binary, weights_binary))}\")\n",
    "    print(f\"Label smoothing: 0.1\")\n",
    "else:\n",
    "    ce_loss_binary = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "print(\"Training setup complete!\")\n",
    "print(f\"  Total training steps: {total_steps_binary}\")\n",
    "print(f\"  Warmup steps: {warmup_steps_binary}\")\n",
    "print(f\"  Using AMP: {use_amp_binary}\")\n",
    "print(f\"  Output directory: {OUT_DIR_BINARY}\")\n",
    "\n",
    "# Quick evaluation function for training\n",
    "def run_eval_and_log_binary(split_name, loader):\n",
    "    \"\"\"Evaluate model and log results\"\"\"\n",
    "    metrics, report, cm, class_names = evaluate_model(model_binary, loader, dataset_binary.label_encoder, device)\n",
    "    \n",
    "    # Format metrics nicely\n",
    "    formatted_metrics = {k: f\"{v:.4f}\" if isinstance(v, float) else v for k, v in metrics.items()}\n",
    "    print(f\"{split_name.upper()}: {formatted_metrics}\")\n",
    "    \n",
    "    # Save visualizations\n",
    "    save_confusion_matrix(cm, class_names, OUT_DIR_BINARY / f\"cm_{split_name}.png\", \n",
    "                         normalize=True, title=f\"Binary {split_name.title()} Confusion Matrix\")\n",
    "    save_classification_report(report, OUT_DIR_BINARY / f\"cls_report_{split_name}.csv\")\n",
    "    \n",
    "    # Log to scoreboard\n",
    "    dataset_size = len(loader.dataset) if hasattr(loader, \"dataset\") else len(loader.dataset.dataset) if hasattr(loader.dataset, \"dataset\") else None\n",
    "    append_metrics_row(\"./eval_out/binary_scoreboard.csv\", MODEL_TAG, split_name, metrics, \n",
    "                      extras={f\"n_{split_name}\": dataset_size})\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "181d6b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation on TEST Split:\n",
      "TEST: {'accuracy': '0.5137', 'balanced_accuracy': '0.5012', 'f1_macro': '0.3758', 'f1_weighted': '0.3838', 'precision_macro': '0.5071', 'recall_macro': '0.5012', 'roc_auc_macro': 'nan', 'pr_auc_macro': '0.4848', 'log_loss': '0.6951', 'support': 183}\n",
      "TEST: {'accuracy': '0.5137', 'balanced_accuracy': '0.5012', 'f1_macro': '0.3758', 'f1_weighted': '0.3838', 'precision_macro': '0.5071', 'recall_macro': '0.5012', 'roc_auc_macro': 'nan', 'pr_auc_macro': '0.4848', 'log_loss': '0.6951', 'support': 183}\n",
      "Extracting predictions for analysis...\n",
      "Extracting predictions for analysis...\n",
      "Binary Model Training Complete!\n",
      "Final Results:\n",
      "  Test Accuracy: 0.5137\n",
      "  Test F1-Score: 0.3758\n",
      "  Best Val F1: Not available (run training loop first)\n",
      "Models saved to:\n",
      "  Best: ./osa_wav2vec2_binary_best\n",
      "  Final: ./osa_wav2vec2_binary_final\n",
      "Results saved to: eval_out/wav2vec2-binary-apnea-vs-normal-v2-fixed\n",
      "Binary Model Training Complete!\n",
      "Final Results:\n",
      "  Test Accuracy: 0.5137\n",
      "  Test F1-Score: 0.3758\n",
      "  Best Val F1: Not available (run training loop first)\n",
      "Models saved to:\n",
      "  Best: ./osa_wav2vec2_binary_best\n",
      "  Final: ./osa_wav2vec2_binary_final\n",
      "Results saved to: eval_out/wav2vec2-binary-apnea-vs-normal-v2-fixed\n"
     ]
    }
   ],
   "source": [
    "# Final Evaluation and Model Saving\n",
    "print(\"\\nFinal Evaluation on TEST Split:\")\n",
    "final_metrics = run_eval_and_log_binary(\"test\", test_loader_binary)\n",
    "\n",
    "# Get raw predictions for analysis\n",
    "print(\"Extracting predictions for analysis...\")\n",
    "_, _, _, _, y_true_test_binary, y_pred_test_binary, y_proba_test_binary = evaluate_model(\n",
    "    model_binary, test_loader_binary, dataset_binary.label_encoder, device, return_arrays=True\n",
    ")\n",
    "\n",
    "# Save prediction arrays\n",
    "np.save(OUT_DIR_BINARY / \"y_true_test.npy\", y_true_test_binary)\n",
    "np.save(OUT_DIR_BINARY / \"y_pred_test.npy\", y_pred_test_binary) \n",
    "np.save(OUT_DIR_BINARY / \"y_proba_test.npy\", y_proba_test_binary)\n",
    "\n",
    "# Save metadata\n",
    "with open(OUT_DIR_BINARY / \"class_names.json\", \"w\") as f:\n",
    "    json.dump(label_names_binary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save training configuration\n",
    "with open(OUT_DIR_BINARY / \"training_config.json\", \"w\") as f:\n",
    "    config = {\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"model_tag\": MODEL_TAG,\n",
    "        \"epochs_configured\": EPOCHS,  # Use configured epochs instead of actual trained\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LR,\n",
    "        \"sample_rate\": SAMPLE_RATE,\n",
    "        \"max_seconds\": MAX_SECONDS,\n",
    "        \"use_class_weights\": USE_CLASS_WEIGHTS,\n",
    "        \"augment\": AUGMENT,\n",
    "        \"best_val_f1\": best_val_f1 if 'best_val_f1' in globals() else 0.0,\n",
    "        \"final_test_metrics\": final_metrics\n",
    "    }\n",
    "    json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save final model\n",
    "model_binary.save_pretrained(\"./osa_wav2vec2_binary_final\")\n",
    "processor_binary.save_pretrained(\"./osa_wav2vec2_binary_final\")\n",
    "\n",
    "print(\"Binary Model Training Complete!\")\n",
    "print(f\"Final Results:\")\n",
    "print(f\"  Test Accuracy: {final_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Test F1-Score: {final_metrics['f1_macro']:.4f}\")\n",
    "if 'best_val_f1' in globals():\n",
    "    print(f\"  Best Val F1: {best_val_f1:.4f}\")\n",
    "else:\n",
    "    print(f\"  Best Val F1: Not available (run training loop first)\")\n",
    "print(f\"Models saved to:\")\n",
    "print(f\"  Best: ./osa_wav2vec2_binary_best\")\n",
    "print(f\"  Final: ./osa_wav2vec2_binary_final\")\n",
    "print(f\"Results saved to: {OUT_DIR_BINARY.as_posix()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a2b5ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… seaborn already installed\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for visualization\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    print(\"âœ… seaborn already installed\")\n",
    "except ImportError:\n",
    "    print(\"ðŸ“¦ Installing seaborn...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"seaborn\"])\n",
    "    print(\"âœ… seaborn installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a66f435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRADIENT EXPLOSION FIX\n",
      "==================================================\n",
      "Reloading model with conservative initialization...\n",
      "Reloading model with conservative initialization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\V89\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:334: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying conservative parameter initialization...\n",
      "  Initialized projector.weight: torch.Size([256, 768])\n",
      "  Initialized projector.bias: torch.Size([256])\n",
      "  Initialized classifier.weight: torch.Size([2, 256])\n",
      "  Initialized classifier.bias: torch.Size([2])\n",
      "Fixed Training Setup Complete:\n",
      "  Learning Rate: 5e-08\n",
      "  Batch Size: 2\n",
      "  Epochs: 5\n",
      "  Max Gradient Norm: 0.01\n",
      "  AMP Disabled: True\n",
      "  Ready for stable training!\n"
     ]
    }
   ],
   "source": [
    "# GRADIENT EXPLOSION FIX - Complete Training Setup\n",
    "print(\"GRADIENT EXPLOSION FIX\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Clean up existing model if present\n",
    "if 'model_binary' in globals():\n",
    "    try:\n",
    "        del model_binary\n",
    "    except:\n",
    "        pass\n",
    "if 'optimizer_binary' in globals():\n",
    "    try:\n",
    "        del optimizer_binary\n",
    "    except:\n",
    "        pass\n",
    "if 'scheduler_binary' in globals():\n",
    "    try:\n",
    "        del scheduler_binary\n",
    "    except:\n",
    "        pass  \n",
    "if 'scaler_binary' in globals():\n",
    "    try:\n",
    "        del scaler_binary\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Reload model with better initialization\n",
    "print(\"Reloading model with conservative initialization...\")\n",
    "model_binary = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=n_classes_binary,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "# Configure label mappings\n",
    "model_binary.config.id2label = {i: name for i, name in enumerate(label_names_binary)}\n",
    "model_binary.config.label2id = {name: i for i, name in enumerate(label_names_binary)}\n",
    "model_binary.to(device)\n",
    "\n",
    "# CRITICAL FIX: Very conservative parameter initialization\n",
    "print(\"Applying conservative parameter initialization...\")\n",
    "with torch.no_grad():\n",
    "    for name, param in model_binary.named_parameters():\n",
    "        if 'classifier' in name or 'projector' in name:\n",
    "            if param.dim() > 1:\n",
    "                # Much smaller initialization to prevent explosion\n",
    "                nn.init.xavier_normal_(param, gain=0.01)  # Very small gain\n",
    "            else:\n",
    "                # Zero initialization for biases\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            print(f\"  Initialized {name}: {param.shape}\")\n",
    "\n",
    "# ULTRA CONSERVATIVE HYPERPARAMETERS FOR GRADIENT EXPLOSION\n",
    "LR_FIXED = 5e-8      # 20x smaller learning rate\n",
    "BATCH_SIZE_FIXED = 2  # Very small batch size\n",
    "EPOCHS_FIXED = 5     # Keep 5 epochs\n",
    "MAX_GRAD_NORM = 0.01 # Extremely strict gradient clipping\n",
    "\n",
    "# Training components with ultra conservative settings\n",
    "total_steps_fixed = len(train_loader_binary) * EPOCHS_FIXED\n",
    "warmup_steps_fixed = int(total_steps_fixed * 0.2)  # Longer warmup\n",
    "\n",
    "optimizer_binary = torch.optim.AdamW(\n",
    "    model_binary.parameters(), \n",
    "    lr=LR_FIXED,\n",
    "    weight_decay=0.0001,  # Much smaller weight decay\n",
    "    betas=(0.9, 0.98),    # More stable beta2\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Simple constant scheduler for stability\n",
    "from torch.optim.lr_scheduler import ConstantLR\n",
    "scheduler_binary = ConstantLR(optimizer_binary, factor=1.0)\n",
    "\n",
    "# Disable AMP to avoid gradient scaling issues\n",
    "use_amp_binary = False\n",
    "scaler_binary = None\n",
    "\n",
    "# Ultra conservative loss function - no class weights\n",
    "ce_loss_binary = nn.CrossEntropyLoss(label_smoothing=0.05)  # Minimal smoothing\n",
    "\n",
    "# Recreate data loaders with smaller batch size\n",
    "train_loader_fixed = DataLoader(\n",
    "    train_ds_binary, batch_size=BATCH_SIZE_FIXED, shuffle=True,\n",
    "    collate_fn=collate_fn_binary, num_workers=0, pin_memory=False\n",
    ")\n",
    "val_loader_fixed = DataLoader(\n",
    "    val_ds_binary, batch_size=BATCH_SIZE_FIXED, shuffle=False,\n",
    "    collate_fn=collate_fn_binary, num_workers=0, pin_memory=False\n",
    ")\n",
    "\n",
    "print(\"Fixed Training Setup Complete:\")\n",
    "print(f\"  Learning Rate: {LR_FIXED}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE_FIXED}\")\n",
    "print(f\"  Epochs: {EPOCHS_FIXED}\")\n",
    "print(f\"  Max Gradient Norm: {MAX_GRAD_NORM}\")\n",
    "print(f\"  AMP Disabled: {not use_amp_binary}\")\n",
    "print(f\"  Ready for stable training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f47f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Binary Model Training with Gradient Explosion Protection...\n",
      "Configuration:\n",
      "  Model: facebook/wav2vec2-base\n",
      "  Classes: [np.str_('Apnea'), np.str_('Normal')]\n",
      "  Epochs: 5\n",
      "  Batch Size: 2\n",
      "  Learning Rate: 5e-08\n",
      "  Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4988c8e449104726a01e9aa6e21432d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EPOCH 1] Summary:\n",
      "  Steps taken: 730/730\n",
      "  Skipped batches: 0\n",
      "  Gradient explosions: 0\n",
      "  Gradient clipping events: 723\n",
      "  Average gradient norm: 0.405906\n",
      "  Average training loss: 0.693147\n",
      "VAL: {'accuracy': '0.5000', 'balanced_accuracy': '0.5058', 'f1_macro': '0.4870', 'f1_weighted': '0.4843', 'precision_macro': '0.5066', 'recall_macro': '0.5058', 'roc_auc_macro': 'nan', 'pr_auc_macro': '0.4986', 'log_loss': '0.6931', 'support': 182}\n",
      "VAL: {'accuracy': '0.5000', 'balanced_accuracy': '0.5058', 'f1_macro': '0.4870', 'f1_weighted': '0.4843', 'precision_macro': '0.5066', 'recall_macro': '0.5058', 'roc_auc_macro': 'nan', 'pr_auc_macro': '0.4986', 'log_loss': '0.6931', 'support': 182}\n",
      "  Val Accuracy: 0.5000\n",
      "  Val F1-Macro: 0.4870\n",
      "  Learning Rate: 5.00e-08\n",
      "NEW BEST F1: 0.4870\n",
      "  Val Accuracy: 0.5000\n",
      "  Val F1-Macro: 0.4870\n",
      "  Learning Rate: 5.00e-08\n",
      "NEW BEST F1: 0.4870\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c6a30e5f9f42368f5a53e22f6c47a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[EPOCH 2] Summary:\n",
      "  Steps taken: 730/730\n",
      "  Skipped batches: 0\n",
      "  Gradient explosions: 0\n",
      "  Gradient clipping events: 727\n",
      "  Average gradient norm: 0.366619\n",
      "  Average training loss: 0.693146\n",
      "VAL: {'accuracy': '0.4725', 'balanced_accuracy': '0.4861', 'f1_macro': '0.3747', 'f1_weighted': '0.3665', 'precision_macro': '0.4567', 'recall_macro': '0.4861', 'roc_auc_macro': 'nan', 'pr_auc_macro': '0.5091', 'log_loss': '0.6931', 'support': 182}\n",
      "  Val Accuracy: 0.4725\n",
      "  Val F1-Macro: 0.3747\n",
      "  Learning Rate: 5.00e-08\n",
      "Patience: 1/3\n",
      "VAL: {'accuracy': '0.4725', 'balanced_accuracy': '0.4861', 'f1_macro': '0.3747', 'f1_weighted': '0.3665', 'precision_macro': '0.4567', 'recall_macro': '0.4861', 'roc_auc_macro': 'nan', 'pr_auc_macro': '0.5091', 'log_loss': '0.6931', 'support': 182}\n",
      "  Val Accuracy: 0.4725\n",
      "  Val F1-Macro: 0.3747\n",
      "  Learning Rate: 5.00e-08\n",
      "Patience: 1/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23717a5f75aa4cffb5db5237fefd71b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# COMPLETE TRAINING LOOP - Anti Gradient Explosion\n",
    "print(\"Starting Binary Model Training with Gradient Explosion Protection...\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Classes: {label_names_binary}\")\n",
    "print(f\"  Epochs: {EPOCHS_FIXED}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE_FIXED}\")\n",
    "print(f\"  Learning Rate: {LR_FIXED}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "# Training tracking\n",
    "best_val_f1 = 0.0\n",
    "patience_counter = 0\n",
    "patience = 3\n",
    "gradient_explosion_count = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS_FIXED + 1):\n",
    "    model_binary.train()\n",
    "    pbar = tqdm(train_loader_fixed, desc=f\"Epoch {epoch}/{EPOCHS_FIXED}\")\n",
    "    \n",
    "    # Tracking metrics\n",
    "    running_loss = 0.0\n",
    "    batch_count = 0\n",
    "    grad_norm_sum = 0.0\n",
    "    steps_taken = 0\n",
    "    gradient_clipped_count = 0\n",
    "    skipped_batches = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer_binary.zero_grad(set_to_none=True)\n",
    "        \n",
    "        try:\n",
    "            # Forward pass (no AMP for stability)\n",
    "            outputs = model_binary(\n",
    "                input_values=batch[\"input_values\"],\n",
    "                attention_mask=batch.get(\"attention_mask\", None)\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            loss = ce_loss_binary(logits, batch[\"labels\"])\n",
    "            \n",
    "            # Check loss validity\n",
    "            if not torch.isfinite(loss):\n",
    "                print(f\"Skipping batch {batch_idx}: invalid loss {loss.item()}\")\n",
    "                skipped_batches += 1\n",
    "                continue\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # EXTREME gradient checking for explosion detection\n",
    "            total_norm = 0.0\n",
    "            max_param_grad = 0.0\n",
    "            has_inf_grad = False\n",
    "            has_nan_grad = False\n",
    "            \n",
    "            for name, param in model_binary.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    param_grad_norm = param.grad.data.norm(2).item()\n",
    "                    \n",
    "                    # Check for inf/nan gradients\n",
    "                    if torch.isinf(param.grad).any():\n",
    "                        has_inf_grad = True\n",
    "                    if torch.isnan(param.grad).any():\n",
    "                        has_nan_grad = True\n",
    "                    \n",
    "                    if torch.isfinite(torch.tensor(param_grad_norm)):\n",
    "                        total_norm += param_grad_norm ** 2\n",
    "                        max_param_grad = max(max_param_grad, param_grad_norm)\n",
    "            \n",
    "            total_norm = total_norm ** 0.5\n",
    "            \n",
    "            # Handle gradient explosion\n",
    "            if has_inf_grad or has_nan_grad or total_norm > 1000.0:\n",
    "                print(f\"Gradient explosion detected in batch {batch_idx} (norm={total_norm:.2f})\")\n",
    "                gradient_explosion_count += 1\n",
    "                skipped_batches += 1\n",
    "                \n",
    "                # If too many explosions, stop training\n",
    "                if gradient_explosion_count > 10:\n",
    "                    print(\"Too many gradient explosions. Stopping training.\")\n",
    "                    break\n",
    "                continue\n",
    "            \n",
    "            # Apply extremely strict gradient clipping\n",
    "            if total_norm > MAX_GRAD_NORM:\n",
    "                torch.nn.utils.clip_grad_norm_(model_binary.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "                gradient_clipped_count += 1\n",
    "            \n",
    "            # Take optimization step only if gradients are safe\n",
    "            optimizer_binary.step()\n",
    "            scheduler_binary.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            steps_taken += 1\n",
    "            grad_norm_sum += min(total_norm, 10.0)  # Cap for averaging\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {e}\")\n",
    "            skipped_batches += 1\n",
    "            continue\n",
    "        \n",
    "        # Update progress bar\n",
    "        if batch_count > 0:\n",
    "            avg_loss = running_loss / batch_count\n",
    "            avg_grad_norm = grad_norm_sum / max(1, steps_taken)\n",
    "            current_lr = optimizer_binary.param_groups[0]['lr']\n",
    "            \n",
    "            pbar.set_postfix(\n",
    "                loss=f\"{avg_loss:.4f}\",\n",
    "                grad_norm=f\"{avg_grad_norm:.4f}\",\n",
    "                lr=f\"{current_lr:.2e}\",\n",
    "                steps=f\"{steps_taken}/{len(train_loader_fixed)}\",\n",
    "                skipped=skipped_batches\n",
    "            )\n",
    "    \n",
    "    # Check if too many gradient explosions occurred\n",
    "    if gradient_explosion_count > 10:\n",
    "        print(\"Training stopped due to excessive gradient explosions\")\n",
    "        break\n",
    "    \n",
    "    # Epoch summary\n",
    "    print(f\"\\n[EPOCH {epoch}] Summary:\")\n",
    "    print(f\"  Steps taken: {steps_taken}/{len(train_loader_fixed)}\")\n",
    "    print(f\"  Skipped batches: {skipped_batches}\")\n",
    "    print(f\"  Gradient explosions: {gradient_explosion_count}\")\n",
    "    print(f\"  Gradient clipping events: {gradient_clipped_count}\")\n",
    "    \n",
    "    if steps_taken > 0:\n",
    "        avg_grad_norm = grad_norm_sum / steps_taken\n",
    "        avg_loss = running_loss / batch_count if batch_count > 0 else float('inf')\n",
    "        print(f\"  Average gradient norm: {avg_grad_norm:.6f}\")\n",
    "        print(f\"  Average training loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    if steps_taken == 0:\n",
    "        print(\"WARNING: No steps taken! Training failed.\")\n",
    "        break\n",
    "    \n",
    "    # Validation\n",
    "    val_metrics = run_eval_and_log_binary(\"val\", val_loader_fixed)\n",
    "    val_f1 = val_metrics['f1_macro']\n",
    "    val_accuracy = val_metrics['accuracy']\n",
    "    \n",
    "    print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"  Val F1-Macro: {val_f1:.4f}\")\n",
    "    print(f\"  Learning Rate: {current_lr:.2e}\")\n",
    "    \n",
    "    # Track best model\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        print(f\"NEW BEST F1: {best_val_f1:.4f}\")\n",
    "        model_binary.save_pretrained(\"./osa_wav2vec2_binary_best\")\n",
    "        processor_binary.save_pretrained(\"./osa_wav2vec2_binary_best\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Patience: {patience_counter}/{patience}\")\n",
    "    \n",
    "    if patience_counter >= patience and epoch >= 3:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTRAINING COMPLETE!\")\n",
    "print(f\"Best Validation F1: {best_val_f1:.4f}\")\n",
    "print(f\"Total Gradient Explosions: {gradient_explosion_count}\")\n",
    "\n",
    "# Generate all required variables for evaluation\n",
    "print(\"\\nGenerating final test predictions...\")\n",
    "\n",
    "# Final evaluation to get all required variables\n",
    "final_metrics, final_report, final_cm, class_names_binary = evaluate_model(\n",
    "    model_binary, test_loader_binary, dataset_binary.label_encoder, device\n",
    ")\n",
    "\n",
    "# Get prediction arrays for detailed analysis\n",
    "_, _, _, _, y_true_test_binary, y_pred_test_binary, y_proba_test_binary = evaluate_model(\n",
    "    model_binary, test_loader_binary, dataset_binary.label_encoder, device, return_arrays=True\n",
    ")\n",
    "\n",
    "print(\"All variables generated for detailed evaluation!\")\n",
    "print(f\"Final Test Metrics:\")\n",
    "print(f\"  Accuracy: {final_metrics['accuracy']:.4f}\")\n",
    "print(f\"  F1-Macro: {final_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "# Save final model\n",
    "model_binary.save_pretrained(\"./osa_wav2vec2_binary_final\")\n",
    "processor_binary.save_pretrained(\"./osa_wav2vec2_binary_final\")\n",
    "\n",
    "print(f\"\\nModels saved:\")\n",
    "print(f\"  Best: ./osa_wav2vec2_binary_best\")\n",
    "print(f\"  Final: ./osa_wav2vec2_binary_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea7eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE DETAILED BINARY MODEL EVALUATION\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BINARY MODEL DETAILED EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create enhanced confusion matrix visualization\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Handle division by zero in normalization\n",
    "test_cm_binary = final_cm\n",
    "row_sums = test_cm_binary.sum(axis=1, keepdims=True)\n",
    "row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "cm_normalized = test_cm_binary.astype('float') / row_sums\n",
    "\n",
    "# Create beautiful heatmap with seaborn\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues', \n",
    "            xticklabels=class_names_binary, yticklabels=class_names_binary,\n",
    "            cbar_kws={'label': 'Normalized Frequency'},\n",
    "            square=True, linewidths=0.5, annot_kws={'size': 14})\n",
    "\n",
    "plt.title('Binary Classification Confusion Matrix\\n(Apnea vs Normal)', \n",
    "          fontsize=18, fontweight='bold', pad=25)\n",
    "plt.xlabel('Predicted Class', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Class', fontsize=14, fontweight='bold')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12, rotation=0)\n",
    "\n",
    "# Add count annotations\n",
    "for i in range(len(class_names_binary)):\n",
    "    for j in range(len(class_names_binary)):\n",
    "        count = test_cm_binary[i, j]\n",
    "        plt.text(j + 0.5, i + 0.7, f'({count})', \n",
    "                ha='center', va='center', fontsize=12, color='darkred', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./binary_confusion_matrix_detailed.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Detailed Classification Report\n",
    "print(\"\\nBINARY CLASSIFICATION REPORT:\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_true_test_binary, y_pred_test_binary, \n",
    "                          target_names=class_names_binary, digits=4))\n",
    "\n",
    "# Binary-specific metrics\n",
    "if y_proba_test_binary.shape[1] == 2:\n",
    "    roc_auc = roc_auc_score(y_true_test_binary, y_proba_test_binary[:, 1])\n",
    "    pr_auc = average_precision_score(y_true_test_binary, y_proba_test_binary[:, 1])\n",
    "    \n",
    "    print(\"\\nBINARY METRICS:\")\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "    print(f\"PR-AUC Score: {pr_auc:.4f}\")\n",
    "\n",
    "# Per-class confidence analysis\n",
    "print(\"\\nCONFIDENCE ANALYSIS:\")\n",
    "print(\"=\" * 35)\n",
    "for i, class_name in enumerate(class_names_binary):\n",
    "    class_probs = y_proba_test_binary[:, i]\n",
    "    class_mask = (y_true_test_binary == i)\n",
    "    \n",
    "    if class_mask.sum() > 0:\n",
    "        avg_prob_correct = class_probs[class_mask].mean()\n",
    "        avg_prob_incorrect = class_probs[~class_mask].mean()\n",
    "        confidence_gap = avg_prob_correct - avg_prob_incorrect\n",
    "        \n",
    "        print(f\"{class_name}:\")\n",
    "        print(f\"  Avg confidence when correct: {avg_prob_correct:.4f}\")\n",
    "        print(f\"  Avg confidence when wrong: {avg_prob_incorrect:.4f}\")\n",
    "        print(f\"  Confidence gap: {confidence_gap:.4f}\")\n",
    "\n",
    "# Final Model Summary\n",
    "print(\"\\nBINARY MODEL FINAL SUMMARY:\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Classes: {class_names_binary}\")\n",
    "print(f\"Train/Val/Test: {len(train_ds_binary)}/{len(val_ds_binary)}/{len(test_ds_binary)}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Final Test Accuracy: {final_metrics['accuracy']:.4f}\")\n",
    "print(f\"Final Test F1-Score: {final_metrics['f1_macro']:.4f}\")\n",
    "print(f\"Best Validation F1: {best_val_f1:.4f}\")\n",
    "print(f\"Training Epochs: {EPOCHS_FIXED}\")\n",
    "print(f\"Learning Rate: {LR_FIXED}\")\n",
    "\n",
    "print(\"\\nBinary Classification Analysis Complete!\")\n",
    "print(\"Ready for production use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
