{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "340d6480",
   "metadata": {},
   "source": [
    "# Model\n",
    "### Wav2Vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "02279059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f32204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, re, random, numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import torchaudio\n",
    "    HAVE_TA = True\n",
    "except Exception:\n",
    "    HAVE_TA = False\n",
    "    print(\"[WARN] torchaudio not found, augmentation disabled.\")\n",
    "\n",
    "import pyedflib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_recall_fscore_support,\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    log_loss, average_precision_score\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor, Wav2Vec2ForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "848799c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# Prefer CUDA (your RTX 4060). Fallback to CPU if unavailable.\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9a616e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = r\"C:\\V89\\Snore_Apnea_Analyze\\EDF_RML\\data_csv\\respiratory_plus_normal.csv\"\n",
    "EDF_ROOT = r\"C:\\V89\\data2\"\n",
    "\n",
    "MODEL_NAME   = \"facebook/wav2vec2-base\"\n",
    "MODEL_TAG    = \"wav2vec2-base-osa-win4060-v2\"  # เปลี่ยนชื่อใหม่\n",
    "\n",
    "SAMPLE_RATE  = 16000\n",
    "MAX_SECONDS  = 8      \n",
    "BATCH_SIZE   = 4\n",
    "EPOCHS       = 5      # เพิ่ม epochs\n",
    "LR           = 1e-5   # ลด learning rate\n",
    "WARMUP_RATIO = 0.1\n",
    "FREEZE_BASE  = False  # **เปลี่ยนเป็น False - นี่คือปัญหาหลัก**\n",
    "UNFREEZE_EPOCH = None # ปิดการใช้งาน\n",
    "AUGMENT      = True        \n",
    "USE_CLASS_WEIGHTS = True   # เปิดใช้ class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eedb048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleepApneaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expect df with columns: patient_id, type, segment_index, segment_local_start_sec, duration_sec, ...\n",
    "    Resolve EDF path once and drop rows without matches.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, edf_root: str, sample_rate: int = 16000,\n",
    "                 prefer_audio_channels=(\"sound\",\"snore\",\"tracheal\",\"microphone\",\"audio\",\"throat\")):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.edf_root = Path(edf_root)\n",
    "        self.prefer_audio_channels = tuple(s.lower() for s in prefer_audio_channels)\n",
    "\n",
    "        df = df.copy()\n",
    "        df[\"pid_str\"]   = df[\"patient_id\"].astype(str)\n",
    "        df[\"pid_unpad\"] = df[\"pid_str\"].str.lstrip(\"0\")\n",
    "        df[\"pid_pad8\"]  = df[\"pid_unpad\"].str.zfill(8)\n",
    "        df[\"seg3\"]      = df[\"segment_index\"].astype(int).map(lambda x: f\"{x:03d}\")\n",
    "\n",
    "        def resolve_row(row):\n",
    "            pid_unpad = row[\"pid_unpad\"]\n",
    "            pid_pad8  = row[\"pid_pad8\"]\n",
    "            seg3      = row[\"seg3\"]\n",
    "            patterns = [\n",
    "                f\"*{pid_pad8}*{seg3}*.edf\",\n",
    "                f\"*{pid_unpad}*{seg3}*.edf\",\n",
    "                f\"*{pid_pad8}*.edf\",\n",
    "                f\"*{pid_unpad}*.edf\",\n",
    "            ]\n",
    "            for pat in patterns:\n",
    "                hits = list(self.edf_root.rglob(pat))\n",
    "                if len(hits) == 1:\n",
    "                    return hits[0]\n",
    "                if len(hits) > 1:\n",
    "                    ranked = sorted(\n",
    "                        hits,\n",
    "                        key=lambda p: (\n",
    "                            0 if re.search(rf\"{seg3}\\b\", p.stem) else 1,\n",
    "                            0 if re.search(r\"(snore|sound|trach|mic|psg|audio|throat)\", p.stem.lower()) else 1,\n",
    "                            len(p.as_posix())\n",
    "                        )\n",
    "                    )\n",
    "                    return ranked[0]\n",
    "            return None\n",
    "\n",
    "        df[\"edf_path\"] = df.apply(resolve_row, axis=1)\n",
    "        missing = df[\"edf_path\"].isna().sum()\n",
    "        if missing:\n",
    "            print(f\"[WARNING] Skipping {missing} rows that have no matching EDF file.\")\n",
    "        df = df.dropna(subset=[\"edf_path\"]).reset_index(drop=True)\n",
    "\n",
    "        self.df = df\n",
    "        self.labels = sorted(self.df['type'].unique().tolist())\n",
    "        self.label_encoder = LabelEncoder().fit(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _pick_channel_index(self, f: pyedflib.EdfReader):\n",
    "        labels = [s.lower() for s in f.getSignalLabels()]\n",
    "        for i, name in enumerate(labels):\n",
    "            if any(key in name for key in self.prefer_audio_channels):\n",
    "                return i\n",
    "        return 0  # fallback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        edf_path = Path(row[\"edf_path\"])\n",
    "        start_sec = float(row['segment_local_start_sec'])\n",
    "        duration_sec = float(row['duration_sec'])\n",
    "        label = self.label_encoder.transform([row['type']])[0]\n",
    "\n",
    "        f = pyedflib.EdfReader(str(edf_path))\n",
    "        ch_idx = self._pick_channel_index(f)\n",
    "        signal = f.readSignal(ch_idx)\n",
    "        fs = f.getSampleFrequency(ch_idx)\n",
    "        f.close()\n",
    "\n",
    "        start_sample = max(0, int(start_sec * fs))\n",
    "        end_sample   = min(int((start_sec + duration_sec) * fs), len(signal))\n",
    "        audio = signal[start_sample:end_sample]\n",
    "\n",
    "        x = torch.tensor(audio, dtype=torch.float32)\n",
    "        if fs != self.sample_rate:\n",
    "            if not HAVE_TA:\n",
    "                raise RuntimeError(\"torchaudio is required for resampling\")\n",
    "            x = torchaudio.transforms.Resample(fs, self.sample_rate)(x)\n",
    "\n",
    "        return {\"audio\": x, \"label\": int(label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fce49281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Skipping 245 rows that have no matching EDF file.\n",
      "Resolved EDF rows: 1824\n",
      "   patient_id pid_unpad  pid_pad8  segment_index  \\\n",
      "0         999       999  00000999              0   \n",
      "1         999       999  00000999              0   \n",
      "2         999       999  00000999              0   \n",
      "3         999       999  00000999              0   \n",
      "4         999       999  00000999              0   \n",
      "5         999       999  00000999              0   \n",
      "6         999       999  00000999              0   \n",
      "7         999       999  00000999              0   \n",
      "\n",
      "                                edf_path  \n",
      "0  C:\\V89\\data2\\00000999-100507[001].edf  \n",
      "1  C:\\V89\\data2\\00000999-100507[001].edf  \n",
      "2  C:\\V89\\data2\\00000999-100507[001].edf  \n",
      "3  C:\\V89\\data2\\00000999-100507[001].edf  \n",
      "4  C:\\V89\\data2\\00000999-100507[001].edf  \n",
      "5  C:\\V89\\data2\\00000999-100507[001].edf  \n",
      "6  C:\\V89\\data2\\00000999-100507[001].edf  \n",
      "7  C:\\V89\\data2\\00000999-100507[001].edf  \n",
      "Classes: [np.str_('CentralApnea'), np.str_('Hypopnea'), np.str_('MixedApnea'), np.str_('Normal'), np.str_('ObstructiveApnea')]  | n_classes = 5\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "dataset = SleepApneaDataset(df, edf_root=EDF_ROOT, sample_rate=SAMPLE_RATE)\n",
    "\n",
    "print(\"Resolved EDF rows:\", len(dataset))\n",
    "print(dataset.df[[\"patient_id\",\"pid_unpad\",\"pid_pad8\",\"segment_index\",\"edf_path\"]].head(8))\n",
    "\n",
    "label_names = list(dataset.label_encoder.classes_)\n",
    "n_classes = len(label_names)\n",
    "print(\"Classes:\", label_names, \" | n_classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6e97517f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: 1459 182 183\n"
     ]
    }
   ],
   "source": [
    "if SPLIT_FILE.exists():\n",
    "    with open(SPLIT_FILE, \"r\") as f:\n",
    "        idx = json.load(f)\n",
    "    train_idx, val_idx, test_idx = idx[\"train\"], idx[\"val\"], idx[\"test\"]\n",
    "else:\n",
    "    g = torch.Generator().manual_seed(SEED)\n",
    "    N = len(dataset)\n",
    "    perm = torch.randperm(N, generator=g).tolist()\n",
    "    train_ratio, val_ratio = 0.8, 0.1\n",
    "    n_train = int(train_ratio * N)\n",
    "    n_val   = int(val_ratio   * N)\n",
    "    train_idx = perm[:n_train]\n",
    "    val_idx   = perm[n_train:n_train+n_val]\n",
    "    test_idx  = perm[n_train+n_val:]\n",
    "    SPLIT_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(SPLIT_FILE, \"w\") as f:\n",
    "        json.dump({\"train\": train_idx, \"val\": val_idx, \"test\": test_idx}, f, indent=2)\n",
    "\n",
    "train_ds = Subset(dataset, train_idx)\n",
    "val_ds   = Subset(dataset, val_idx)\n",
    "test_ds  = Subset(dataset, test_idx)\n",
    "\n",
    "print(\"Split sizes:\", len(train_ds), len(val_ds), len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4173410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2Vec2Collator:\n",
    "    \"\"\"Prepare raw waveforms for Wav2Vec2 (padding + attention mask + optional aug).\"\"\"\n",
    "    def __init__(self, processor, sr=16000, max_seconds=8, augment=False):\n",
    "        self.processor = processor\n",
    "        self.sr = sr\n",
    "        self.max_len = int(max_seconds * sr)\n",
    "        self.augment = augment and HAVE_TA\n",
    "\n",
    "    def _augment(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.augment: return x\n",
    "        # Gaussian noise\n",
    "        if random.random() < 0.5:\n",
    "            noise_std = 0.005 * (x.abs().mean().item() + 1e-6)\n",
    "            x = x + torch.randn_like(x) * noise_std\n",
    "        # Random gain\n",
    "        if random.random() < 0.3:\n",
    "            gain_db = random.uniform(-3.0, 3.0)\n",
    "            x = x * (10.0 ** (gain_db / 20.0))\n",
    "        return x\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        waves, labels = [], []\n",
    "        for b in batch:\n",
    "            w = b[\"audio\"]\n",
    "            if isinstance(w, np.ndarray):\n",
    "                w = torch.from_numpy(w)\n",
    "            w = w.float().view(-1)\n",
    "\n",
    "            # Crop long clips\n",
    "            if len(w) > self.max_len:\n",
    "                start = random.randint(0, len(w) - self.max_len)\n",
    "                w = w[start:start + self.max_len]\n",
    "\n",
    "            # Peak normalize\n",
    "            peak = w.abs().max()\n",
    "            if peak > 0:\n",
    "                w = w / peak\n",
    "\n",
    "            # Light augmentation\n",
    "            w = self._augment(w)\n",
    "\n",
    "            waves.append(w.numpy())\n",
    "            labels.append(int(b[\"label\"]))\n",
    "\n",
    "        inputs = processor(\n",
    "            waves,\n",
    "            sampling_rate=self.sr,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=False\n",
    "        )\n",
    "        inputs[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fb139b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\V89\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:334: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Parameters Status ===\n",
      "✓ TRAINABLE: wav2vec2.masked_spec_embed - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.feature_extractor.conv_layers.0.conv.weight - torch.Size([512, 1, 10])\n",
      "✓ TRAINABLE: wav2vec2.feature_extractor.conv_layers.0.layer_norm.weight - torch.Size([512])\n",
      "✓ TRAINABLE: wav2vec2.feature_extractor.conv_layers.0.layer_norm.bias - torch.Size([512])\n",
      "✓ TRAINABLE: wav2vec2.feature_extractor.conv_layers.1.conv.weight - torch.Size([512, 512, 3])\n",
      "✓ TRAINABLE: wav2vec2.feature_extractor.conv_layers.2.conv.weight - torch.Size([512, 512, 3])\n",
      "✓ TRAINABLE: wav2vec2.feature_extractor.conv_layers.3.conv.weight - torch.Size([512, 512, 3])\n",
      "✓ TRAINABLE: wav2vec2.feature_extractor.conv_layers.4.conv.weight - torch.Size([512, 512, 3])\n",
      "✓ TRAINABLE: wav2vec2.feature_extractor.conv_layers.5.conv.weight - torch.Size([512, 512, 2])\n",
      "✓ TRAINABLE: wav2vec2.feature_extractor.conv_layers.6.conv.weight - torch.Size([512, 512, 2])\n",
      "✓ TRAINABLE: wav2vec2.feature_projection.layer_norm.weight - torch.Size([512])\n",
      "✓ TRAINABLE: wav2vec2.feature_projection.layer_norm.bias - torch.Size([512])\n",
      "✓ TRAINABLE: wav2vec2.feature_projection.projection.weight - torch.Size([768, 512])\n",
      "✓ TRAINABLE: wav2vec2.feature_projection.projection.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.pos_conv_embed.conv.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0 - torch.Size([1, 1, 128])\n",
      "✓ TRAINABLE: wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1 - torch.Size([768, 48, 128])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.0.attention.k_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.0.attention.k_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.0.attention.v_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.0.attention.v_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.0.attention.q_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.0.attention.q_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.0.attention.out_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.0.attention.out_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.0.layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.0.layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight - torch.Size([3072, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias - torch.Size([3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.0.feed_forward.output_dense.weight - torch.Size([768, 3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.0.feed_forward.output_dense.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.0.final_layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.0.final_layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.1.attention.k_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.1.attention.k_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.1.attention.v_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.1.attention.v_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.1.attention.q_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.1.attention.q_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.1.attention.out_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.1.attention.out_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.1.layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.1.layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight - torch.Size([3072, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias - torch.Size([3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.1.feed_forward.output_dense.weight - torch.Size([768, 3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.1.feed_forward.output_dense.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.1.final_layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.1.final_layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.2.attention.k_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.2.attention.k_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.2.attention.v_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.2.attention.v_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.2.attention.q_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.2.attention.q_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.2.attention.out_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.2.attention.out_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.2.layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.2.layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight - torch.Size([3072, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias - torch.Size([3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.2.feed_forward.output_dense.weight - torch.Size([768, 3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.2.feed_forward.output_dense.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.2.final_layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.2.final_layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.3.attention.k_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.3.attention.k_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.3.attention.v_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.3.attention.v_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.3.attention.q_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.3.attention.q_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.3.attention.out_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.3.attention.out_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.3.layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.3.layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight - torch.Size([3072, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias - torch.Size([3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.3.feed_forward.output_dense.weight - torch.Size([768, 3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.3.feed_forward.output_dense.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.3.final_layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.3.final_layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.4.attention.k_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.4.attention.k_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.4.attention.v_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.4.attention.v_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.4.attention.q_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.4.attention.q_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.4.attention.out_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.4.attention.out_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.4.layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.4.layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight - torch.Size([3072, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias - torch.Size([3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.4.feed_forward.output_dense.weight - torch.Size([768, 3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.4.feed_forward.output_dense.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.4.final_layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.4.final_layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.5.attention.k_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.5.attention.k_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.5.attention.v_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.5.attention.v_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.5.attention.q_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.5.attention.q_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.5.attention.out_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.5.attention.out_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.5.layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.5.layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight - torch.Size([3072, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias - torch.Size([3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.5.feed_forward.output_dense.weight - torch.Size([768, 3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.5.feed_forward.output_dense.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.5.final_layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.5.final_layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.6.attention.k_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.6.attention.k_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.6.attention.v_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.6.attention.v_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.6.attention.q_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.6.attention.q_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.6.attention.out_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.6.attention.out_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.6.layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.6.layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight - torch.Size([3072, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias - torch.Size([3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.6.feed_forward.output_dense.weight - torch.Size([768, 3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.6.feed_forward.output_dense.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.6.final_layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.6.final_layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.7.attention.k_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.7.attention.k_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.7.attention.v_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.7.attention.v_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.7.attention.q_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.7.attention.q_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.7.attention.out_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.7.attention.out_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.7.layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.7.layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight - torch.Size([3072, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias - torch.Size([3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.7.feed_forward.output_dense.weight - torch.Size([768, 3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.7.feed_forward.output_dense.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.7.final_layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.7.final_layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.8.attention.k_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.8.attention.k_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.8.attention.v_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.8.attention.v_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.8.attention.q_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.8.attention.q_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.8.attention.out_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.8.attention.out_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.8.layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.8.layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight - torch.Size([3072, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias - torch.Size([3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.8.feed_forward.output_dense.weight - torch.Size([768, 3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.8.feed_forward.output_dense.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.8.final_layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.8.final_layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.9.attention.k_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.9.attention.k_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.9.attention.v_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.9.attention.v_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.9.attention.q_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.9.attention.q_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.9.attention.out_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.9.attention.out_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.9.layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.9.layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight - torch.Size([3072, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias - torch.Size([3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.9.feed_forward.output_dense.weight - torch.Size([768, 3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.9.feed_forward.output_dense.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.9.final_layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.9.final_layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.10.attention.k_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.10.attention.k_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.10.attention.v_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.10.attention.v_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.10.attention.q_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.10.attention.q_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.10.attention.out_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.10.attention.out_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.10.layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.10.layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight - torch.Size([3072, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias - torch.Size([3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.10.feed_forward.output_dense.weight - torch.Size([768, 3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.10.feed_forward.output_dense.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.10.final_layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.10.final_layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.11.attention.k_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.11.attention.k_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.11.attention.v_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.11.attention.v_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.11.attention.q_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.11.attention.q_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.11.attention.out_proj.weight - torch.Size([768, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.11.attention.out_proj.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.11.layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.11.layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight - torch.Size([3072, 768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias - torch.Size([3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.11.feed_forward.output_dense.weight - torch.Size([768, 3072])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.11.feed_forward.output_dense.bias - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.11.final_layer_norm.weight - torch.Size([768])\n",
      "✓ TRAINABLE: wav2vec2.encoder.layers.11.final_layer_norm.bias - torch.Size([768])\n",
      "✓ TRAINABLE: projector.weight - torch.Size([256, 768])\n",
      "✓ TRAINABLE: projector.bias - torch.Size([256])\n",
      "✓ TRAINABLE: classifier.weight - torch.Size([5, 256])\n",
      "✓ TRAINABLE: classifier.bias - torch.Size([5])\n",
      "\n",
      "Trainable parameters: 94,569,861\n",
      "Frozen parameters: 0\n",
      "Total parameters: 94,569,861\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=n_classes,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "# Label maps in config\n",
    "model.config.id2label = {i: name for i, name in enumerate(label_names)}\n",
    "model.config.label2id = {name: i for i, name in enumerate(label_names)}\n",
    "\n",
    "# ลบส่วน freeze ออก หรือใช้ partial freeze\n",
    "# if FREEZE_BASE:\n",
    "#     model.freeze_feature_encoder()\n",
    "\n",
    "# แทนที่ด้วย: Partial freeze (ถ้าต้องการ)\n",
    "if FREEZE_BASE:\n",
    "    # Freeze เฉพาะส่วน feature extractor แต่ปล่อย projection และ classifier\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'feature_extractor' in name:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# เพิ่มการตรวจสอบ parameters\n",
    "print(\"=== Model Parameters Status ===\")\n",
    "trainable_params = 0\n",
    "frozen_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "        print(f\"✓ TRAINABLE: {name} - {param.shape}\")\n",
    "    else:\n",
    "        frozen_params += param.numel()\n",
    "        print(f\"✗ FROZEN: {name} - {param.shape}\")\n",
    "\n",
    "print(f\"\\nTrainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {frozen_params:,}\")\n",
    "print(f\"Total parameters: {trainable_params + frozen_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1c8a78f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = Wav2Vec2Collator(processor, sr=SAMPLE_RATE, max_seconds=MAX_SECONDS, augment=AUGMENT)\n",
    "\n",
    "# For notebooks on Windows, keep num_workers=0 (pyedflib may not be multiprocess-safe in all setups)\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = (device.type == \"cuda\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=collate_fn, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "57f80e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, label_encoder, device, return_arrays=False):\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_proba_chunks = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(\n",
    "                input_values=batch[\"input_values\"],\n",
    "                attention_mask=batch.get(\"attention_mask\", None)\n",
    "            ).logits\n",
    "            proba = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            y_proba_chunks.append(proba)\n",
    "            y_pred.extend(np.argmax(proba, axis=1))\n",
    "            y_true.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_proba = np.vstack(y_proba_chunks) if len(y_proba_chunks) > 0 else np.zeros((0, len(label_encoder.classes_)))\n",
    "\n",
    "    class_names = list(label_encoder.classes_)\n",
    "    n_classes = len(class_names)\n",
    "    \n",
    "    # Fix: Use only labels that appear in predictions\n",
    "    unique_labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=unique_labels)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred) if len(y_true) else np.nan\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0) if len(y_true) else np.nan\n",
    "    f1_weighted = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0) if len(y_true) else np.nan\n",
    "    prec_macro, rec_macro, _, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0) if len(y_true) else (np.nan, np.nan, None, None)\n",
    "\n",
    "    cm_float = cm.astype(float) if cm.size else np.zeros((len(unique_labels), len(unique_labels)), float)\n",
    "    row_sums = cm_float.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1.0\n",
    "    bal_acc = (cm_float / row_sums).diagonal().mean() if cm.size else np.nan\n",
    "\n",
    "    try:\n",
    "        roc_auc_macro = roc_auc_score(y_true, y_proba, multi_class=\"ovr\", average=\"macro\")\n",
    "    except Exception:\n",
    "        roc_auc_macro = np.nan\n",
    "    try:\n",
    "        pr_auc_macro = average_precision_score(np.eye(n_classes)[y_true], y_proba, average=\"macro\")\n",
    "    except Exception:\n",
    "        pr_auc_macro = np.nan\n",
    "    try:\n",
    "        ll = log_loss(y_true, y_proba, labels=list(range(n_classes)))\n",
    "    except Exception:\n",
    "        ll = np.nan\n",
    "\n",
    "    # Fix: Use target_names that match actual labels in predictions\n",
    "    report_dict = classification_report(\n",
    "        y_true, y_pred, \n",
    "        labels=unique_labels,\n",
    "        target_names=[class_names[i] for i in unique_labels],\n",
    "        output_dict=True, zero_division=0\n",
    "    ) if len(y_true) else {}\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": acc,\n",
    "        \"balanced_accuracy\": bal_acc,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"precision_macro\": prec_macro,\n",
    "        \"recall_macro\": rec_macro,\n",
    "        \"roc_auc_macro\": roc_auc_macro,\n",
    "        \"pr_auc_macro\": pr_auc_macro,\n",
    "        \"log_loss\": ll,\n",
    "        \"support\": int(len(y_true))\n",
    "    }\n",
    "    out = (metrics, report_dict, cm, class_names)\n",
    "    if return_arrays:\n",
    "        return out + (y_true, y_pred, y_proba)\n",
    "    return out\n",
    "\n",
    "def save_confusion_matrix(cm, class_names, save_path, normalize=True, title=None):\n",
    "    cm_plot = cm.astype(float)\n",
    "    if normalize and cm_plot.size:\n",
    "        row_sums = cm_plot.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1.0\n",
    "        cm_plot = cm_plot / row_sums\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_plot, interpolation='nearest')\n",
    "    plt.title(title or \"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    ticks = np.arange(len(class_names))\n",
    "    plt.xticks(ticks, class_names, rotation=45, ha=\"right\"); plt.yticks(ticks, class_names)\n",
    "\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm_plot.max() / 2. if cm_plot.size else 0.5\n",
    "    for i in range(cm_plot.shape[0]):\n",
    "        for j in range(cm_plot.shape[1]):\n",
    "            val = cm_plot[i, j] if normalize else int(cm[i, j])\n",
    "            plt.text(j, i, format(val, fmt),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if (cm_plot[i, j] if cm_plot.size else 0) > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "def save_classification_report(report_dict, save_csv_path):\n",
    "    if not report_dict:\n",
    "        return None\n",
    "    df = pd.DataFrame(report_dict).T\n",
    "    Path(save_csv_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(save_csv_path, index=True)\n",
    "    return df\n",
    "\n",
    "def append_metrics_row(results_csv, model_name, split_name, metrics, extras=None):\n",
    "    row = {\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"model\": model_name,\n",
    "        \"split\": split_name,\n",
    "        **metrics\n",
    "    }\n",
    "    if extras:\n",
    "        row.update(extras)\n",
    "    if os.path.exists(results_csv):\n",
    "        df = pd.read_csv(results_csv)\n",
    "        df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame([row])\n",
    "    Path(results_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(results_csv, index=False)\n",
    "    return df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78f6248",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "409294d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(\n",
    "    input_values=batch[\"input_values\"],\n",
    "    attention_mask=batch.get(\"attention_mask\", None)\n",
    ").logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9502cc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abcde\\AppData\\Local\\Temp\\ipykernel_12772\\2653787837.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler  = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Optimizer created with current model parameters\n",
      "Optimizer parameter groups: 1\n",
      "Parameters in optimizer: 215\n"
     ]
    }
   ],
   "source": [
    "# แก้ไขเซลล์ #VSC-9cc5bbe8\n",
    "# กำหนดตัวแปรที่จำเป็น\n",
    "SPLIT_FILE = Path(\"./splits/data_split.json\")\n",
    "OUT_DIR = Path(f\"./eval_out/{MODEL_TAG}\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ❌ ลบส่วนนี้ออก - เพราะ optimizer ถูกสร้างใหม่หลังจาก model setup\n",
    "# total_steps  = len(train_loader) * EPOCHS\n",
    "# warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "use_amp = (device.type == \"cuda\")\n",
    "scaler  = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "# Optional: class weights\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    all_y = dataset.label_encoder.transform(dataset.df['type'])\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    classes = np.arange(n_classes)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=all_y)\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32, device=device)\n",
    "    ce_loss = nn.CrossEntropyLoss(weight=class_weights)\n",
    "else:\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# ✅ เพิ่ม optimizer ใหม่หลังจาก model ถูก setup แล้ว\n",
    "total_steps  = len(train_loader) * EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "print(\"✅ Optimizer created with current model parameters\")\n",
    "print(f\"Optimizer parameter groups: {len(optimizer.param_groups)}\")\n",
    "print(f\"Parameters in optimizer: {sum(len(group['params']) for group in optimizer.param_groups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "de0d66ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abcde\\AppData\\Local\\Temp\\ipykernel_12772\\2766179936.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler  = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
      "Epoch 1/5:   0%|          | 0/365 [00:00<?, ?it/s]c:\\V89\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n",
      "Epoch 1/5:   0%|          | 1/365 [00:01<07:04,  1.16s/it, grad_norm=212015.7188, loss=1.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid gradient in wav2vec2.feature_extractor.conv_layers.0.conv.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   1%|          | 2/365 [00:02<05:56,  1.02it/s, grad_norm=159076.9062, loss=3.22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid gradient in wav2vec2.feature_extractor.conv_layers.0.conv.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   1%|          | 3/365 [00:02<05:35,  1.08it/s, grad_norm=60327.4648, loss=2.41] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid gradient in wav2vec2.feature_extractor.conv_layers.0.conv.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   2%|▏         | 7/365 [00:06<05:16,  1.13it/s, grad_norm=28171.0527, loss=1.87] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid gradient in wav2vec2.feature_extractor.conv_layers.0.conv.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   3%|▎         | 12/365 [00:11<06:01,  1.02s/it, grad_norm=15760.1182, loss=1.77] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid gradient in wav2vec2.feature_extractor.conv_layers.0.conv.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  24%|██▍       | 89/365 [01:15<03:47,  1.21it/s, grad_norm=11012.3047, loss=1.62] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid gradient in wav2vec2.feature_extractor.conv_layers.0.conv.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 365/365 [05:19<00:00,  1.14it/s, grad_norm=10126.9072, loss=1.5]  \n",
      "c:\\V89\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL: {'accuracy': 0.14835164835164835, 'balanced_accuracy': np.float64(0.25), 'f1_macro': 0.0645933014354067, 'f1_weighted': 0.03833009096166991, 'precision_macro': 0.03708791208791209, 'recall_macro': 0.25, 'roc_auc_macro': nan, 'pr_auc_macro': 0.23161138017724978, 'log_loss': 1.29908391846995, 'support': 182}\n",
      "Epoch 1 - Val Accuracy: 0.1484, F1: 0.0646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 365/365 [05:06<00:00,  1.19it/s, grad_norm=10694.7520, loss=1.45]\n",
      "c:\\V89\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL: {'accuracy': 0.46153846153846156, 'balanced_accuracy': np.float64(0.25789141414141414), 'f1_macro': 0.1945378151260504, 'f1_weighted': 0.3283682703850771, 'precision_macro': 0.17125748502994012, 'recall_macro': 0.25789141414141414, 'roc_auc_macro': nan, 'pr_auc_macro': 0.21175312006588148, 'log_loss': 1.2328526202206826, 'support': 182}\n",
      "Epoch 2 - Val Accuracy: 0.4615, F1: 0.1945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 365/365 [05:07<00:00,  1.19it/s, grad_norm=9904.5488, loss=1.43] \n",
      "c:\\V89\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL: {'accuracy': 0.32967032967032966, 'balanced_accuracy': np.float64(0.25), 'f1_macro': 0.12396694214876033, 'f1_weighted': 0.16347289074561802, 'precision_macro': 0.08241758241758242, 'recall_macro': 0.25, 'roc_auc_macro': nan, 'pr_auc_macro': 0.2268638454821721, 'log_loss': 1.209533402563697, 'support': 182}\n",
      "Epoch 3 - Val Accuracy: 0.3297, F1: 0.1240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 365/365 [05:16<00:00,  1.15it/s, grad_norm=4151.5908, loss=1.43] \n",
      "c:\\V89\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL: {'accuracy': 0.43956043956043955, 'balanced_accuracy': np.float64(0.24806397306397307), 'f1_macro': 0.21454632324197542, 'f1_weighted': 0.36182594281017605, 'precision_macro': 0.23783875482420797, 'recall_macro': 0.24806397306397307, 'roc_auc_macro': nan, 'pr_auc_macro': 0.21309067931188178, 'log_loss': 1.1992114003940368, 'support': 182}\n",
      "Epoch 4 - Val Accuracy: 0.4396, F1: 0.2145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 365/365 [05:56<00:00,  1.02it/s, grad_norm=9714.7471, loss=1.42] \n",
      "c:\\V89\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL: {'accuracy': 0.3791208791208791, 'balanced_accuracy': np.float64(0.27605218855218855), 'f1_macro': 0.24962330487192363, 'f1_weighted': 0.3521506118191201, 'precision_macro': 0.27989790957705396, 'recall_macro': 0.27605218855218855, 'roc_auc_macro': nan, 'pr_auc_macro': 0.25580397881009, 'log_loss': 1.1907396579370555, 'support': 182}\n",
      "Epoch 5 - Val Accuracy: 0.3791, F1: 0.2496\n",
      "Final evaluation on TEST split:\n",
      "TEST: {'accuracy': 0.33879781420765026, 'balanced_accuracy': np.float64(0.2022127659574468), 'f1_macro': 0.19676460516402988, 'f1_weighted': 0.3407357007885155, 'precision_macro': 0.22857142857142856, 'recall_macro': 0.2022127659574468, 'roc_auc_macro': 0.5468200331791401, 'pr_auc_macro': 0.23682813281293966, 'log_loss': 1.21849509574613, 'support': 183}\n",
      "Done. Artifacts saved to: eval_out/wav2vec2-base-osa-win4060-v2\n"
     ]
    }
   ],
   "source": [
    "# แทนที่เซลล์ที่เลือกด้วยโค้ดนี้\n",
    "\n",
    "total_steps  = len(train_loader) * EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "use_amp = (device.type == \"cuda\")\n",
    "scaler  = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "# Optional: class weights\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    all_y = dataset.label_encoder.transform(dataset.df['type'])\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    classes = np.arange(n_classes)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=classes, y=all_y)\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32, device=device)\n",
    "    ce_loss = nn.CrossEntropyLoss(weight=class_weights)\n",
    "else:\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# เพิ่มเซลล์ใหม่สำหรับ training loop\n",
    "def run_eval_and_log(split_name, loader):\n",
    "    metrics, report, cm, class_names = evaluate_model(model, loader, dataset.label_encoder, device)\n",
    "    print(f\"{split_name.upper()}:\", metrics)\n",
    "    save_confusion_matrix(cm, class_names, OUT_DIR / f\"cm_{split_name}.png\", normalize=True, title=f\"{split_name.title()} CM (norm)\")\n",
    "    save_classification_report(report, OUT_DIR / f\"cls_report_{split_name}.csv\")\n",
    "    append_metrics_row(\"./eval_out/scoreboard.csv\", MODEL_TAG, split_name, metrics, extras={f\"n_{split_name}\": len(loader.dataset) if hasattr(loader, \"dataset\") else None})\n",
    "    return metrics\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    running = 0.0\n",
    "\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if use_amp:\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(\n",
    "                    input_values=batch[\"input_values\"],\n",
    "                    attention_mask=batch.get(\"attention_mask\", None)\n",
    "                )\n",
    "                logits = outputs.logits\n",
    "                loss = ce_loss(logits, batch[\"labels\"])\n",
    "            \n",
    "            # ✅ เช็คว่า loss เป็น nan หรือไม่\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"Skipping batch due to invalid loss: {loss.item()}\")\n",
    "                continue\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # ✅ ตรวจสอบ gradients อย่างละเอียด\n",
    "            total_norm = 0\n",
    "            valid_grads = 0\n",
    "            for name, p in model.named_parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    if torch.isfinite(param_norm):\n",
    "                        total_norm += param_norm ** 2\n",
    "                        valid_grads += 1\n",
    "                    else:\n",
    "                        print(f\"Invalid gradient in {name}\")\n",
    "            \n",
    "            total_norm = total_norm ** 0.5\n",
    "            \n",
    "            if valid_grads > 0 and torch.isfinite(total_norm) and total_norm > 0:\n",
    "                # ✅ ลด gradient clipping threshold\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                print(f\"Skipping optimization step: valid_grads={valid_grads}, grad_norm={total_norm}\")\n",
    "                scaler.update()  # ยังคงต้อง update scaler\n",
    "        else:\n",
    "            outputs = model(\n",
    "                input_values=batch[\"input_values\"],\n",
    "                attention_mask=batch.get(\"attention_mask\", None)\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            loss = ce_loss(logits, batch[\"labels\"])\n",
    "            \n",
    "            # ✅ เช็คว่า loss เป็น nan หรือไม่\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"Skipping batch due to invalid loss: {loss.item()}\")\n",
    "                continue\n",
    "                \n",
    "            loss.backward()\n",
    "            \n",
    "            # ✅ ตรวจสอบ gradients อย่างละเอียด\n",
    "            total_norm = 0\n",
    "            valid_grads = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    if torch.isfinite(param_norm):\n",
    "                        total_norm += param_norm ** 2\n",
    "                        valid_grads += 1\n",
    "            \n",
    "            total_norm = total_norm ** 0.5\n",
    "            \n",
    "            if valid_grads > 0 and torch.isfinite(total_norm) and total_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                print(f\"Skipping optimization step: valid_grads={valid_grads}, grad_norm={total_norm}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        # ✅ ป้องกัน nan ใน running loss\n",
    "        if torch.isfinite(loss):\n",
    "            running += loss.item()\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            loss=running / max(1, pbar.n), \n",
    "            grad_norm=f\"{total_norm:.4f}\" if torch.isfinite(total_norm) else \"inf\"\n",
    "        )\n",
    "\n",
    "    metrics = run_eval_and_log(\"val\", val_loader)\n",
    "    print(f\"Epoch {epoch} - Val Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1_macro']:.4f}\")\n",
    "\n",
    "print(\"Final evaluation on TEST split:\")\n",
    "_ = run_eval_and_log(\"test\", test_loader)\n",
    "\n",
    "# Save raw preds for analysis\n",
    "_, _, _, _, y_true_test, y_pred_test, y_proba_test = evaluate_model(\n",
    "    model, test_loader, dataset.label_encoder, device, return_arrays=True\n",
    ")\n",
    "np.save(OUT_DIR / \"y_true_test.npy\", y_true_test)\n",
    "np.save(OUT_DIR / \"y_pred_test.npy\", y_pred_test)\n",
    "np.save(OUT_DIR / \"y_proba_test.npy\", y_proba_test)\n",
    "with open(OUT_DIR / \"class_names.json\", \"w\") as f:\n",
    "    json.dump(label_names, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save checkpoint\n",
    "model.save_pretrained(\"./osa_wav2vec2_ckpt\")\n",
    "processor.save_pretrained(\"./osa_wav2vec2_ckpt\")\n",
    "print(\"Done. Artifacts saved to:\", OUT_DIR.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3110783",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9ec7b2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATION RESULTS ===\n",
      "\n",
      "📊 VALIDATION SET:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\V89\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3407\n",
      "F1-Macro: 0.1956\n",
      "F1-Weighted: 0.3133\n",
      "\n",
      "🎯 TEST SET:\n",
      "Accuracy: 0.3333\n",
      "F1-Macro: 0.1820\n",
      "F1-Weighted: 0.3267\n"
     ]
    }
   ],
   "source": [
    "# Evaluate ทุก split\n",
    "print(\"=== EVALUATION RESULTS ===\")\n",
    "\n",
    "# Validation\n",
    "print(\"\\n📊 VALIDATION SET:\")\n",
    "val_metrics, val_report, val_cm, class_names = evaluate_model(\n",
    "    model, val_loader, dataset.label_encoder, device\n",
    ")\n",
    "print(f\"Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "print(f\"F1-Macro: {val_metrics['f1_macro']:.4f}\")\n",
    "print(f\"F1-Weighted: {val_metrics['f1_weighted']:.4f}\")\n",
    "\n",
    "# Test\n",
    "print(\"\\n🎯 TEST SET:\")\n",
    "test_metrics, test_report, test_cm, _ = evaluate_model(\n",
    "    model, test_loader, dataset.label_encoder, device\n",
    ")\n",
    "print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"F1-Macro: {test_metrics['f1_macro']:.4f}\")\n",
    "print(f\"F1-Weighted: {test_metrics['f1_weighted']:.4f}\")\n",
    "\n",
    "# Save visualizations\n",
    "save_confusion_matrix(val_cm, class_names, \"./eval_results/cm_val.png\", \n",
    "                     normalize=True, title=\"Validation Confusion Matrix\")\n",
    "save_confusion_matrix(test_cm, class_names, \"./eval_results/cm_test.png\", \n",
    "                     normalize=True, title=\"Test Confusion Matrix\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
